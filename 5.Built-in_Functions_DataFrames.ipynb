{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder.master('local').appName('operDataFrames').enableHiveSupport().getOrCreate()\nsc=spark.sparkContext",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders = spark.read.csv('data//retail_db//orders.csv',header=True,inferSchema=True)\norder_items = spark.read.csv('data//retail_db//order_items.csv',header=True,inferSchema=True)\ncustomers = spark.read.csv('data//retail_db//customers.csv',header=True,inferSchema=True)\nproducts = spark.read.csv('data//retail_db//products.csv',header=True,inferSchema=True)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Built-in functions "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Most of the functions are from pyspark.sql.functions and also dataframe built-in functions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pyspark.sql.functions as F\nhelp(F)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Help on module pyspark.sql.functions in pyspark.sql:\n\nNAME\n    pyspark.sql.functions - A collections of builtin functions\n\nCLASSES\n    builtins.object\n        builtins.str\n        PandasUDFType\n    \n    class PandasUDFType(builtins.object)\n     |  Pandas UDF Types. See :meth:`pyspark.sql.functions.pandas_udf`.\n     |  \n     |  Data descriptors defined here:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  GROUPED_AGG = 202\n     |  \n     |  GROUPED_MAP = 201\n     |  \n     |  SCALAR = 200\n    \n    basestring = class str(object)\n     |  str(object='') -> str\n     |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n     |  \n     |  Create a new string object from the given object. If encoding or\n     |  errors is specified, then the object must expose a data buffer\n     |  that will be decoded using the given encoding and error handler.\n     |  Otherwise, returns the result of object.__str__() (if defined)\n     |  or repr(object).\n     |  encoding defaults to sys.getdefaultencoding().\n     |  errors defaults to 'strict'.\n     |  \n     |  Methods defined here:\n     |  \n     |  __add__(self, value, /)\n     |      Return self+value.\n     |  \n     |  __contains__(self, key, /)\n     |      Return key in self.\n     |  \n     |  __eq__(self, value, /)\n     |      Return self==value.\n     |  \n     |  __format__(...)\n     |      S.__format__(format_spec) -> str\n     |      \n     |      Return a formatted version of S as described by format_spec.\n     |  \n     |  __ge__(self, value, /)\n     |      Return self>=value.\n     |  \n     |  __getattribute__(self, name, /)\n     |      Return getattr(self, name).\n     |  \n     |  __getitem__(self, key, /)\n     |      Return self[key].\n     |  \n     |  __getnewargs__(...)\n     |  \n     |  __gt__(self, value, /)\n     |      Return self>value.\n     |  \n     |  __hash__(self, /)\n     |      Return hash(self).\n     |  \n     |  __iter__(self, /)\n     |      Implement iter(self).\n     |  \n     |  __le__(self, value, /)\n     |      Return self<=value.\n     |  \n     |  __len__(self, /)\n     |      Return len(self).\n     |  \n     |  __lt__(self, value, /)\n     |      Return self<value.\n     |  \n     |  __mod__(self, value, /)\n     |      Return self%value.\n     |  \n     |  __mul__(self, value, /)\n     |      Return self*value.\n     |  \n     |  __ne__(self, value, /)\n     |      Return self!=value.\n     |  \n     |  __new__(*args, **kwargs) from builtins.type\n     |      Create and return a new object.  See help(type) for accurate signature.\n     |  \n     |  __repr__(self, /)\n     |      Return repr(self).\n     |  \n     |  __rmod__(self, value, /)\n     |      Return value%self.\n     |  \n     |  __rmul__(self, value, /)\n     |      Return value*self.\n     |  \n     |  __sizeof__(...)\n     |      S.__sizeof__() -> size of S in memory, in bytes\n     |  \n     |  __str__(self, /)\n     |      Return str(self).\n     |  \n     |  capitalize(...)\n     |      S.capitalize() -> str\n     |      \n     |      Return a capitalized version of S, i.e. make the first character\n     |      have upper case and the rest lower case.\n     |  \n     |  casefold(...)\n     |      S.casefold() -> str\n     |      \n     |      Return a version of S suitable for caseless comparisons.\n     |  \n     |  center(...)\n     |      S.center(width[, fillchar]) -> str\n     |      \n     |      Return S centered in a string of length width. Padding is\n     |      done using the specified fill character (default is a space)\n     |  \n     |  count(...)\n     |      S.count(sub[, start[, end]]) -> int\n     |      \n     |      Return the number of non-overlapping occurrences of substring sub in\n     |      string S[start:end].  Optional arguments start and end are\n     |      interpreted as in slice notation.\n     |  \n     |  encode(...)\n     |      S.encode(encoding='utf-8', errors='strict') -> bytes\n     |      \n     |      Encode S using the codec registered for encoding. Default encoding\n     |      is 'utf-8'. errors may be given to set a different error\n     |      handling scheme. Default is 'strict' meaning that encoding errors raise\n     |      a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and\n     |      'xmlcharrefreplace' as well as any other name registered with\n     |      codecs.register_error that can handle UnicodeEncodeErrors.\n     |  \n     |  endswith(...)\n     |      S.endswith(suffix[, start[, end]]) -> bool\n     |      \n     |      Return True if S ends with the specified suffix, False otherwise.\n     |      With optional start, test S beginning at that position.\n     |      With optional end, stop comparing S at that position.\n     |      suffix can also be a tuple of strings to try.\n     |  \n     |  expandtabs(...)\n     |      S.expandtabs(tabsize=8) -> str\n     |      \n     |      Return a copy of S where all tab characters are expanded using spaces.\n     |      If tabsize is not given, a tab size of 8 characters is assumed.\n     |  \n     |  find(...)\n     |      S.find(sub[, start[, end]]) -> int\n     |      \n     |      Return the lowest index in S where substring sub is found,\n     |      such that sub is contained within S[start:end].  Optional\n     |      arguments start and end are interpreted as in slice notation.\n     |      \n     |      Return -1 on failure.\n     |  \n     |  format(...)\n     |      S.format(*args, **kwargs) -> str\n     |      \n     |      Return a formatted version of S, using substitutions from args and kwargs.\n     |      The substitutions are identified by braces ('{' and '}').\n     |  \n     |  format_map(...)\n     |      S.format_map(mapping) -> str\n     |      \n     |      Return a formatted version of S, using substitutions from mapping.\n     |      The substitutions are identified by braces ('{' and '}').\n     |  \n     |  index(...)\n     |      S.index(sub[, start[, end]]) -> int\n     |      \n     |      Return the lowest index in S where substring sub is found, \n     |      such that sub is contained within S[start:end].  Optional\n     |      arguments start and end are interpreted as in slice notation.\n     |      \n     |      Raises ValueError when the substring is not found.\n     |  \n     |  isalnum(...)\n     |      S.isalnum() -> bool\n     |      \n     |      Return True if all characters in S are alphanumeric\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  isalpha(...)\n     |      S.isalpha() -> bool\n     |      \n     |      Return True if all characters in S are alphabetic\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  isdecimal(...)\n     |      S.isdecimal() -> bool\n     |      \n     |      Return True if there are only decimal characters in S,\n     |      False otherwise.\n     |  \n     |  isdigit(...)\n     |      S.isdigit() -> bool\n     |      \n     |      Return True if all characters in S are digits\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  isidentifier(...)\n     |      S.isidentifier() -> bool\n     |      \n     |      Return True if S is a valid identifier according\n     |      to the language definition.\n     |      \n     |      Use keyword.iskeyword() to test for reserved identifiers\n     |      such as \"def\" and \"class\".\n     |  \n     |  islower(...)\n     |      S.islower() -> bool\n     |      \n     |      Return True if all cased characters in S are lowercase and there is\n     |      at least one cased character in S, False otherwise.\n     |  \n     |  isnumeric(...)\n     |      S.isnumeric() -> bool\n     |      \n     |      Return True if there are only numeric characters in S,\n     |      False otherwise.\n     |  \n     |  isprintable(...)\n     |      S.isprintable() -> bool\n     |      \n     |      Return True if all characters in S are considered\n     |      printable in repr() or S is empty, False otherwise.\n     |  \n     |  isspace(...)\n     |      S.isspace() -> bool\n     |      \n     |      Return True if all characters in S are whitespace\n     |      and there is at least one character in S, False otherwise.\n     |  \n     |  istitle(...)\n     |      S.istitle() -> bool\n     |      \n     |      Return True if S is a titlecased string and there is at least one\n     |      character in S, i.e. upper- and titlecase characters may only\n     |      follow uncased characters and lowercase characters only cased ones.\n     |      Return False otherwise.\n     |  \n     |  isupper(...)\n     |      S.isupper() -> bool\n     |      \n     |      Return True if all cased characters in S are uppercase and there is\n     |      at least one cased character in S, False otherwise.\n     |  \n     |  join(...)\n     |      S.join(iterable) -> str\n     |      \n     |      Return a string which is the concatenation of the strings in the\n     |      iterable.  The separator between elements is S.\n     |  \n     |  ljust(...)\n     |      S.ljust(width[, fillchar]) -> str\n     |      \n     |      Return S left-justified in a Unicode string of length width. Padding is\n     |      done using the specified fill character (default is a space).\n     |  \n     |  lower(...)\n     |      S.lower() -> str\n     |      \n     |      Return a copy of the string S converted to lowercase.\n     |  \n     |  lstrip(...)\n     |      S.lstrip([chars]) -> str\n     |      \n     |      Return a copy of the string S with leading whitespace removed.\n     |      If chars is given and not None, remove characters in chars instead.\n     |  \n     |  partition(...)\n     |      S.partition(sep) -> (head, sep, tail)\n     |      \n     |      Search for the separator sep in S, and return the part before it,\n     |      the separator itself, and the part after it.  If the separator is not\n     |      found, return S and two empty strings.\n     |  \n     |  replace(...)\n     |      S.replace(old, new[, count]) -> str\n     |      \n     |      Return a copy of S with all occurrences of substring\n     |      old replaced by new.  If the optional argument count is\n     |      given, only the first count occurrences are replaced.\n     |  \n     |  rfind(...)\n     |      S.rfind(sub[, start[, end]]) -> int\n     |      \n     |      Return the highest index in S where substring sub is found,\n     |      such that sub is contained within S[start:end].  Optional\n     |      arguments start and end are interpreted as in slice notation.\n     |      \n     |      Return -1 on failure.\n     |  \n     |  rindex(...)\n     |      S.rindex(sub[, start[, end]]) -> int\n     |      \n     |      Return the highest index in S where substring sub is found,\n     |      such that sub is contained within S[start:end].  Optional\n     |      arguments start and end are interpreted as in slice notation.\n     |      \n     |      Raises ValueError when the substring is not found.\n     |  \n     |  rjust(...)\n     |      S.rjust(width[, fillchar]) -> str\n     |      \n     |      Return S right-justified in a string of length width. Padding is\n     |      done using the specified fill character (default is a space).\n     |  \n     |  rpartition(...)\n     |      S.rpartition(sep) -> (head, sep, tail)\n     |      \n     |      Search for the separator sep in S, starting at the end of S, and return\n     |      the part before it, the separator itself, and the part after it.  If the\n     |      separator is not found, return two empty strings and S.\n     |  \n     |  rsplit(...)\n     |      S.rsplit(sep=None, maxsplit=-1) -> list of strings\n     |      \n     |      Return a list of the words in S, using sep as the\n     |      delimiter string, starting at the end of the string and\n     |      working to the front.  If maxsplit is given, at most maxsplit\n     |      splits are done. If sep is not specified, any whitespace string\n     |      is a separator.\n     |  \n     |  rstrip(...)\n     |      S.rstrip([chars]) -> str\n     |      \n     |      Return a copy of the string S with trailing whitespace removed.\n     |      If chars is given and not None, remove characters in chars instead.\n     |  \n     |  split(...)\n     |      S.split(sep=None, maxsplit=-1) -> list of strings\n     |      \n     |      Return a list of the words in S, using sep as the\n     |      delimiter string.  If maxsplit is given, at most maxsplit\n     |      splits are done. If sep is not specified or is None, any\n     |      whitespace string is a separator and empty strings are\n     |      removed from the result.\n     |  \n     |  splitlines(...)\n     |      S.splitlines([keepends]) -> list of strings\n     |      \n     |      Return a list of the lines in S, breaking at line boundaries.\n     |      Line breaks are not included in the resulting list unless keepends\n     |      is given and true.\n     |  \n     |  startswith(...)\n     |      S.startswith(prefix[, start[, end]]) -> bool\n     |      \n     |      Return True if S starts with the specified prefix, False otherwise.\n     |      With optional start, test S beginning at that position.\n     |      With optional end, stop comparing S at that position.\n     |      prefix can also be a tuple of strings to try.\n     |  \n     |  strip(...)\n     |      S.strip([chars]) -> str\n     |      \n     |      Return a copy of the string S with leading and trailing\n     |      whitespace removed.\n     |      If chars is given and not None, remove characters in chars instead.\n     |  \n     |  swapcase(...)\n     |      S.swapcase() -> str\n     |      \n     |      Return a copy of S with uppercase characters converted to lowercase\n     |      and vice versa.\n     |  \n     |  title(...)\n     |      S.title() -> str\n     |      \n     |      Return a titlecased version of S, i.e. words start with title case\n     |      characters, all remaining cased characters have lower case.\n     |  \n     |  translate(...)\n     |      S.translate(table) -> str\n     |      \n     |      Return a copy of the string S in which each character has been mapped\n     |      through the given translation table. The table must implement\n     |      lookup/indexing via __getitem__, for instance a dictionary or list,\n     |      mapping Unicode ordinals to Unicode ordinals, strings, or None. If\n     |      this operation raises LookupError, the character is left untouched.\n     |      Characters mapped to None are deleted.\n     |  \n     |  upper(...)\n     |      S.upper() -> str\n     |      \n     |      Return a copy of S converted to uppercase.\n     |  \n     |  zfill(...)\n     |      S.zfill(width) -> str\n     |      \n     |      Pad a numeric string S with zeros on the left, to fill a field\n     |      of the specified width. The string S is never truncated.\n     |  \n     |  ----------------------------------------------------------------------\n     |  Static methods defined here:\n     |  \n     |  maketrans(x, y=None, z=None, /)\n     |      Return a translation table usable for str.translate().\n     |      \n     |      If there is only one argument, it must be a dictionary mapping Unicode\n     |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n     |      Character keys will be then converted to ordinals.\n     |      If there are two arguments, they must be strings of equal length, and\n     |      in the resulting dictionary, each character in x will be mapped to the\n     |      character at the same position in y. If there is a third argument, it\n     |      must be a string, whose characters will be mapped to None in the result.\n\nFUNCTIONS\n    abs(col)\n        Computes the absolute value.\n        \n        .. versionadded:: 1.3\n    \n    acos(col)\n        :return: inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n        \n        .. versionadded:: 1.4\n    \n    add_months(start, months)\n        Returns the date that is `months` months after `start`\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n        [Row(next_month=datetime.date(2015, 5, 8))]\n        \n        .. versionadded:: 1.5\n    \n    approxCountDistinct(col, rsd=None)\n        .. note:: Deprecated in 2.1, use :func:`approx_count_distinct` instead.\n        \n        .. versionadded:: 1.3\n    \n    approx_count_distinct(col, rsd=None)\n        Aggregate function: returns a new :class:`Column` for approximate distinct count of\n        column `col`.\n        \n        :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n            efficient to use :func:`countDistinct`\n        \n        >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n        [Row(distinct_ages=2)]\n        \n        .. versionadded:: 2.1\n    \n    array(*cols)\n        Creates a new array column.\n        \n        :param cols: list of column names (string) or list of :class:`Column` expressions that have\n            the same data type.\n        \n        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n        \n        .. versionadded:: 1.4\n    \n    array_contains(col, value)\n        Collection function: returns null if the array is null, true if the array contains the\n        given value, and false otherwise.\n        \n        :param col: name of column containing array\n        :param value: value to check for in array\n        \n        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n        >>> df.select(array_contains(df.data, \"a\")).collect()\n        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n        \n        .. versionadded:: 1.5\n    \n    array_distinct(col)\n        Collection function: removes duplicate values from the array.\n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n        >>> df.select(array_distinct(df.data)).collect()\n        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n        \n        .. versionadded:: 2.4\n    \n    array_except(col1, col2)\n        Collection function: returns an array of the elements in col1 but not in col2,\n        without duplicates.\n        \n        :param col1: name of column containing array\n        :param col2: name of column containing array\n        \n        >>> from pyspark.sql import Row\n        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n        >>> df.select(array_except(df.c1, df.c2)).collect()\n        [Row(array_except(c1, c2)=['b'])]\n        \n        .. versionadded:: 2.4\n    \n    array_intersect(col1, col2)\n        Collection function: returns an array of the elements in the intersection of col1 and col2,\n        without duplicates.\n        \n        :param col1: name of column containing array\n        :param col2: name of column containing array\n        \n        >>> from pyspark.sql import Row\n        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n        [Row(array_intersect(c1, c2)=['a', 'c'])]\n        \n        .. versionadded:: 2.4\n    \n    array_join(col, delimiter, null_replacement=None)\n        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n        `null_replacement` if set, otherwise they are ignored.\n        \n        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n        [Row(joined='a,b,c'), Row(joined='a')]\n        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n        \n        .. versionadded:: 2.4\n    \n    array_max(col)\n        Collection function: returns the maximum value of the array.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n        >>> df.select(array_max(df.data).alias('max')).collect()\n        [Row(max=3), Row(max=10)]\n        \n        .. versionadded:: 2.4\n    \n    array_min(col)\n        Collection function: returns the minimum value of the array.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n        >>> df.select(array_min(df.data).alias('min')).collect()\n        [Row(min=1), Row(min=-1)]\n        \n        .. versionadded:: 2.4\n    \n    array_position(col, value)\n        Collection function: Locates the position of the first occurrence of the given value\n        in the given array. Returns null if either of the arguments are null.\n        \n        .. note:: The position is not zero based, but 1 based index. Returns 0 if the given\n            value could not be found in the array.\n        \n        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n        >>> df.select(array_position(df.data, \"a\")).collect()\n        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n        \n        .. versionadded:: 2.4\n    \n    array_remove(col, element)\n        Collection function: Remove all elements that equal to element from the given array.\n        \n        :param col: name of column containing array\n        :param element: element to be removed from the array\n        \n        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n        >>> df.select(array_remove(df.data, 1)).collect()\n        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n        \n        .. versionadded:: 2.4\n    \n    array_repeat(col, count)\n        Collection function: creates an array containing a column repeated count times.\n        \n        >>> df = spark.createDataFrame([('ab',)], ['data'])\n        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n        [Row(r=['ab', 'ab', 'ab'])]\n        \n        .. versionadded:: 2.4\n    \n    array_sort(col)\n        Collection function: sorts the input array in ascending order. The elements of the input array\n        must be orderable. Null elements will be placed at the end of the returned array.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n        >>> df.select(array_sort(df.data).alias('r')).collect()\n        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n        \n        .. versionadded:: 2.4\n    \n    array_union(col1, col2)\n        Collection function: returns an array of the elements in the union of col1 and col2,\n        without duplicates.\n        \n        :param col1: name of column containing array\n        :param col2: name of column containing array\n        \n        >>> from pyspark.sql import Row\n        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n        >>> df.select(array_union(df.c1, df.c2)).collect()\n        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n        \n        .. versionadded:: 2.4\n    \n    arrays_overlap(a1, a2)\n        Collection function: returns true if the arrays contain any common non-null element; if not,\n        returns null if both the arrays are non-empty and any of them contains a null element; returns\n        false otherwise.\n        \n        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n        [Row(overlap=True), Row(overlap=False)]\n        \n        .. versionadded:: 2.4\n    \n    arrays_zip(*cols)\n        Collection function: Returns a merged array of structs in which the N-th struct contains all\n        N-th values of input arrays.\n        \n        :param cols: columns of arrays to be merged.\n        \n        >>> from pyspark.sql.functions import arrays_zip\n        >>> df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n        >>> df.select(arrays_zip(df.vals1, df.vals2).alias('zipped')).collect()\n        [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\n        \n        .. versionadded:: 2.4\n    \n    asc(col)\n        Returns a sort expression based on the ascending order of the given column name.\n        \n        .. versionadded:: 1.3\n    \n    asc_nulls_first(col)\n        Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values.\n        \n        .. versionadded:: 2.4\n    \n    asc_nulls_last(col)\n        Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.\n        \n        .. versionadded:: 2.4\n    \n    ascii(col)\n        Computes the numeric value of the first character of the string column.\n        \n        .. versionadded:: 1.5\n    \n    asin(col)\n        :return: inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n        \n        .. versionadded:: 1.4\n    \n    atan(col)\n        :return: inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n        \n        .. versionadded:: 1.4\n    \n    atan2(col1, col2)\n        :param col1: coordinate on y-axis\n        :param col2: coordinate on x-axis\n        :return: the `theta` component of the point\n           (`r`, `theta`)\n           in polar coordinates that corresponds to the point\n           (`x`, `y`) in Cartesian coordinates,\n           as if computed by `java.lang.Math.atan2()`\n        \n        .. versionadded:: 1.4\n    \n    avg(col)\n        Aggregate function: returns the average of the values in a group.\n        \n        .. versionadded:: 1.3\n    \n    base64(col)\n        Computes the BASE64 encoding of a binary column and returns it as a string column.\n        \n        .. versionadded:: 1.5\n    \n    bin(col)\n        Returns the string representation of the binary value of the given column.\n        \n        >>> df.select(bin(df.age).alias('c')).collect()\n        [Row(c='10'), Row(c='101')]\n        \n        .. versionadded:: 1.5\n    \n    bitwiseNOT(col)\n        Computes bitwise not.\n        \n        .. versionadded:: 1.4\n    \n    broadcast(df)\n        Marks a DataFrame as small enough for use in broadcast joins.\n        \n        .. versionadded:: 1.6\n    \n    bround(col, scale=0)\n        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n        or at integral part when `scale` < 0.\n        \n        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n        [Row(r=2.0)]\n        \n        .. versionadded:: 2.0\n    \n    cbrt(col)\n        Computes the cube-root of the given value.\n        \n        .. versionadded:: 1.4\n    \n    ceil(col)\n        Computes the ceiling of the given value.\n        \n        .. versionadded:: 1.4\n    \n    coalesce(*cols)\n        Returns the first column that is not null.\n        \n        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n        >>> cDf.show()\n        +----+----+\n        |   a|   b|\n        +----+----+\n        |null|null|\n        |   1|null|\n        |null|   2|\n        +----+----+\n        \n        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n        +--------------+\n        |coalesce(a, b)|\n        +--------------+\n        |          null|\n        |             1|\n        |             2|\n        +--------------+\n        \n        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n        +----+----+----------------+\n        |   a|   b|coalesce(a, 0.0)|\n        +----+----+----------------+\n        |null|null|             0.0|\n        |   1|null|             1.0|\n        |null|   2|             0.0|\n        +----+----+----------------+\n        \n        .. versionadded:: 1.4\n    \n    col(col)\n        Returns a :class:`Column` based on the given column name.\n        \n        .. versionadded:: 1.3\n    \n    collect_list(col)\n        Aggregate function: returns a list of objects with duplicates.\n        \n        .. note:: The function is non-deterministic because the order of collected results depends\n            on order of rows which may be non-deterministic after a shuffle.\n        \n        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n        >>> df2.agg(collect_list('age')).collect()\n        [Row(collect_list(age)=[2, 5, 5])]\n        \n        .. versionadded:: 1.6\n    \n    collect_set(col)\n        Aggregate function: returns a set of objects with duplicate elements eliminated.\n        \n        .. note:: The function is non-deterministic because the order of collected results depends\n            on order of rows which may be non-deterministic after a shuffle.\n        \n        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n        >>> df2.agg(collect_set('age')).collect()\n        [Row(collect_set(age)=[5, 2])]\n        \n        .. versionadded:: 1.6\n    \n    column(col)\n        Returns a :class:`Column` based on the given column name.\n        \n        .. versionadded:: 1.3\n    \n    concat(*cols)\n        Concatenates multiple input columns together into a single column.\n        The function works with strings, binary and compatible array columns.\n        \n        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n        [Row(s='abcd123')]\n        \n        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n        >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n        \n        .. versionadded:: 1.5\n    \n    concat_ws(sep, *cols)\n        Concatenates multiple input string columns together into a single string column,\n        using the given separator.\n        \n        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n        [Row(s='abcd-123')]\n        \n        .. versionadded:: 1.5\n    \n    conv(col, fromBase, toBase)\n        Convert a number in a string column from one base to another.\n        \n        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n        [Row(hex='15')]\n        \n        .. versionadded:: 1.5\n    \n    corr(col1, col2)\n        Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1``\n        and ``col2``.\n        \n        >>> a = range(20)\n        >>> b = [2 * x for x in range(20)]\n        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n        [Row(c=1.0)]\n        \n        .. versionadded:: 1.6\n    \n    cos(col)\n        :param col: angle in radians\n        :return: cosine of the angle, as if computed by `java.lang.Math.cos()`.\n        \n        .. versionadded:: 1.4\n    \n    cosh(col)\n        :param col: hyperbolic angle\n        :return: hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n        \n        .. versionadded:: 1.4\n    \n    count(col)\n        Aggregate function: returns the number of items in a group.\n        \n        .. versionadded:: 1.3\n    \n    countDistinct(col, *cols)\n        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n        \n        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n        [Row(c=2)]\n        \n        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n        [Row(c=2)]\n        \n        .. versionadded:: 1.3\n    \n    covar_pop(col1, col2)\n        Returns a new :class:`Column` for the population covariance of ``col1`` and ``col2``.\n        \n        >>> a = [1] * 10\n        >>> b = [1] * 10\n        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n        [Row(c=0.0)]\n        \n        .. versionadded:: 2.0\n    \n    covar_samp(col1, col2)\n        Returns a new :class:`Column` for the sample covariance of ``col1`` and ``col2``.\n        \n        >>> a = [1] * 10\n        >>> b = [1] * 10\n        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n        [Row(c=0.0)]\n        \n        .. versionadded:: 2.0\n    \n    crc32(col)\n        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n        returns the value as a bigint.\n        \n        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n        [Row(crc32=2743272264)]\n        \n        .. versionadded:: 1.5\n    \n    create_map(*cols)\n        Creates a new map column.\n        \n        :param cols: list of column names (string) or list of :class:`Column` expressions that are\n            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n        \n        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n        \n        .. versionadded:: 2.0\n    \n    cume_dist()\n        Window function: returns the cumulative distribution of values within a window partition,\n        i.e. the fraction of rows that are below the current row.\n        \n        .. versionadded:: 1.6\n    \n    current_date()\n        Returns the current date as a :class:`DateType` column.\n        \n        .. versionadded:: 1.5\n    \n    current_timestamp()\n        Returns the current timestamp as a :class:`TimestampType` column.\n    \n    date_add(start, days)\n        Returns the date that is `days` days after `start`\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n        [Row(next_date=datetime.date(2015, 4, 9))]\n        \n        .. versionadded:: 1.5\n    \n    date_format(date, format)\n        Converts a date/timestamp/string to a value of string in the format specified by the date\n        format given by the second argument.\n        \n        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n        pattern letters of the Java class `java.text.SimpleDateFormat` can be used.\n        \n        .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n            specialized implementation.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n        [Row(date='04/08/2015')]\n        \n        .. versionadded:: 1.5\n    \n    date_sub(start, days)\n        Returns the date that is `days` days before `start`\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n        [Row(prev_date=datetime.date(2015, 4, 7))]\n        \n        .. versionadded:: 1.5\n    \n    date_trunc(format, timestamp)\n        Returns timestamp truncated to the unit specified by the format.\n        \n        :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n            'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n        \n        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n        \n        .. versionadded:: 2.3\n    \n    datediff(end, start)\n        Returns the number of days from `start` to `end`.\n        \n        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n        [Row(diff=32)]\n        \n        .. versionadded:: 1.5\n    \n    dayofmonth(col)\n        Extract the day of the month of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(dayofmonth('dt').alias('day')).collect()\n        [Row(day=8)]\n        \n        .. versionadded:: 1.5\n    \n    dayofweek(col)\n        Extract the day of the week of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(dayofweek('dt').alias('day')).collect()\n        [Row(day=4)]\n        \n        .. versionadded:: 2.3\n    \n    dayofyear(col)\n        Extract the day of the year of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(dayofyear('dt').alias('day')).collect()\n        [Row(day=98)]\n        \n        .. versionadded:: 1.5\n    \n    decode(col, charset)\n        Computes the first argument into a string from a binary using the provided character set\n        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n        \n        .. versionadded:: 1.5\n    \n    degrees(col)\n        Converts an angle measured in radians to an approximately equivalent angle\n        measured in degrees.\n        :param col: angle in radians\n        :return: angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n        \n        .. versionadded:: 2.1\n    \n    dense_rank()\n        Window function: returns the rank of rows within a window partition, without any gaps.\n        \n        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n        and had three people tie for second place, you would say that all three were in second\n        place and that the next person came in third. Rank would give me sequential numbers, making\n        the person that came in third place (after the ties) would register as coming in fifth.\n        \n        This is equivalent to the DENSE_RANK function in SQL.\n        \n        .. versionadded:: 1.6\n    \n    desc(col)\n        Returns a sort expression based on the descending order of the given column name.\n        \n        .. versionadded:: 1.3\n    \n    desc_nulls_first(col)\n        Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values.\n        \n        .. versionadded:: 2.4\n    \n    desc_nulls_last(col)\n        Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values\n        \n        .. versionadded:: 2.4\n    \n    element_at(col, extraction)\n        Collection function: Returns element of array at given index in extraction if col is array.\n        Returns value for the given key in extraction if col is map.\n        \n        :param col: name of column containing array or map\n        :param extraction: index to check for in array or key to check for in map\n        \n        .. note:: The position is not zero based, but 1 based index.\n        \n        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n        >>> df.select(element_at(df.data, 1)).collect()\n        [Row(element_at(data, 1)='a'), Row(element_at(data, 1)=None)]\n        \n        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], ['data'])\n        >>> df.select(element_at(df.data, \"a\")).collect()\n        [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\n        \n        .. versionadded:: 2.4\n    \n    encode(col, charset)\n        Computes the first argument into a binary from a string using the provided character set\n        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n        \n        .. versionadded:: 1.5\n    \n    exp(col)\n        Computes the exponential of the given value.\n        \n        .. versionadded:: 1.4\n    \n    explode(col)\n        Returns a new row for each element in the given array or map.\n        \n        >>> from pyspark.sql import Row\n        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n        \n        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n        +---+-----+\n        |key|value|\n        +---+-----+\n        |  a|    b|\n        +---+-----+\n        \n        .. versionadded:: 1.4\n    \n    explode_outer(col)\n        Returns a new row for each element in the given array or map.\n        Unlike explode, if the array/map is null or empty then null is produced.\n        \n        >>> df = spark.createDataFrame(\n        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n        ...     (\"id\", \"an_array\", \"a_map\")\n        ... )\n        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n        +---+----------+----+-----+\n        | id|  an_array| key|value|\n        +---+----------+----+-----+\n        |  1|[foo, bar]|   x|  1.0|\n        |  2|        []|null| null|\n        |  3|      null|null| null|\n        +---+----------+----+-----+\n        \n        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n        +---+----------+----+\n        | id|     a_map| col|\n        +---+----------+----+\n        |  1|[x -> 1.0]| foo|\n        |  1|[x -> 1.0]| bar|\n        |  2|        []|null|\n        |  3|      null|null|\n        +---+----------+----+\n        \n        .. versionadded:: 2.3\n    \n    expm1(col)\n        Computes the exponential of the given value minus one.\n        \n        .. versionadded:: 1.4\n    \n    expr(str)\n        Parses the expression string into the column that it represents\n        \n        >>> df.select(expr(\"length(name)\")).collect()\n        [Row(length(name)=5), Row(length(name)=3)]\n        \n        .. versionadded:: 1.5\n    \n    factorial(col)\n        Computes the factorial of the given value.\n        \n        >>> df = spark.createDataFrame([(5,)], ['n'])\n        >>> df.select(factorial(df.n).alias('f')).collect()\n        [Row(f=120)]\n        \n        .. versionadded:: 1.5\n    \n    first(col, ignorenulls=False)\n        Aggregate function: returns the first value in a group.\n        \n        The function by default returns the first values it sees. It will return the first non-null\n        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n        \n        .. note:: The function is non-deterministic because its results depends on order of rows which\n            may be non-deterministic after a shuffle.\n        \n        .. versionadded:: 1.3\n    \n    flatten(col)\n        Collection function: creates a single array from an array of arrays.\n        If a structure of nested arrays is deeper than two levels,\n        only one level of nesting is removed.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n        >>> df.select(flatten(df.data).alias('r')).collect()\n        [Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]\n        \n        .. versionadded:: 2.4\n    \n    floor(col)\n        Computes the floor of the given value.\n        \n        .. versionadded:: 1.4\n    \n    format_number(col, d)\n        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n        with HALF_EVEN round mode, and returns the result as a string.\n        \n        :param col: the column name of the numeric value to be formatted\n        :param d: the N decimal places\n        \n        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n        [Row(v='5.0000')]\n        \n        .. versionadded:: 1.5\n    \n    format_string(format, *cols)\n        Formats the arguments in printf-style and returns the result as a string column.\n        \n        :param format: string that can contain embedded format tags and used as result column's value\n        :param cols: list of column names (string) or list of :class:`Column` expressions to\n            be used in formatting\n        \n        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n        [Row(v='5 hello')]\n        \n        .. versionadded:: 1.5\n    \n    from_json(col, schema, options={})\n        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n        as keys type, :class:`StructType` or :class:`ArrayType` with\n        the specified schema. Returns `null`, in the case of an unparseable string.\n        \n        :param col: string column in json format\n        :param schema: a StructType or ArrayType of StructType to use when parsing the json column.\n        :param options: options to control parsing. accepts the same options as the json datasource\n        \n        .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also\n                  supported for ``schema``.\n        \n        >>> from pyspark.sql.types import *\n        >>> data = [(1, '''{\"a\": 1}''')]\n        >>> schema = StructType([StructField(\"a\", IntegerType())])\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n        [Row(json=Row(a=1))]\n        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n        [Row(json=Row(a=1))]\n        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n        [Row(json={'a': 1})]\n        >>> data = [(1, '''[{\"a\": 1}]''')]\n        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n        [Row(json=[Row(a=1)])]\n        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n        [Row(json=Row(a=1))]\n        >>> data = [(1, '''[1, 2, 3]''')]\n        >>> schema = ArrayType(IntegerType())\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n        [Row(json=[1, 2, 3])]\n        \n        .. versionadded:: 2.1\n    \n    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n        representing the timestamp of that moment in the current system time zone in the given\n        format.\n        \n        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n        [Row(ts='2015-04-08 00:00:00')]\n        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n        \n        .. versionadded:: 1.5\n    \n    from_utc_timestamp(timestamp, tz)\n        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n        renders that timestamp as a timestamp in the given time zone.\n        \n        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n        the given timezone.\n        \n        This function may return confusing result if the input is a string with timezone, e.g.\n        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n        according to the timezone in the string, and finally display the result by converting the\n        timestamp to string according to the session local timezone.\n        \n        :param timestamp: the column that contains timestamps\n        :param tz: a string that has the ID of timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc\n        \n        .. versionchanged:: 2.4\n           `tz` can take a :class:`Column` containing timezone ID strings.\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n        \n        .. versionadded:: 1.5\n    \n    get_json_object(col, path)\n        Extracts json object from a json string based on json path specified, and returns json string\n        of the extracted json object. It will return null if the input json string is invalid.\n        \n        :param col: string column in json format\n        :param path: path to the json object to extract\n        \n        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n        \n        .. versionadded:: 1.6\n    \n    greatest(*cols)\n        Returns the greatest value of the list of column names, skipping null values.\n        This function takes at least 2 parameters. It will return null iff all parameters are null.\n        \n        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n        [Row(greatest=4)]\n        \n        .. versionadded:: 1.5\n    \n    grouping(col)\n        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n        \n        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n        +-----+--------------+--------+\n        | name|grouping(name)|sum(age)|\n        +-----+--------------+--------+\n        | null|             1|       7|\n        |Alice|             0|       2|\n        |  Bob|             0|       5|\n        +-----+--------------+--------+\n        \n        .. versionadded:: 2.0\n    \n    grouping_id(*cols)\n        Aggregate function: returns the level of grouping, equals to\n        \n           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n        \n        .. note:: The list of columns should match with grouping columns exactly, or empty (means all\n            the grouping columns).\n        \n        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n        +-----+-------------+--------+\n        | name|grouping_id()|sum(age)|\n        +-----+-------------+--------+\n        | null|            1|       7|\n        |Alice|            0|       2|\n        |  Bob|            0|       5|\n        +-----+-------------+--------+\n        \n        .. versionadded:: 2.0\n    \n    hash(*cols)\n        Calculates the hash code of given columns, and returns the result as an int column.\n        \n        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n        [Row(hash=-757602832)]\n        \n        .. versionadded:: 2.0\n    \n    hex(col)\n        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n        :class:`pyspark.sql.types.LongType`.\n        \n        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n        [Row(hex(a)='414243', hex(b)='3')]\n        \n        .. versionadded:: 1.5\n    \n    hour(col)\n        Extract the hours of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n        >>> df.select(hour('ts').alias('hour')).collect()\n        [Row(hour=13)]\n        \n        .. versionadded:: 1.5\n    \n    hypot(col1, col2)\n        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n        \n        .. versionadded:: 1.4\n    \n    initcap(col)\n        Translate the first letter of each word to upper case in the sentence.\n        \n        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n        [Row(v='Ab Cd')]\n        \n        .. versionadded:: 1.5\n    \n    input_file_name()\n        Creates a string column for the file name of the current Spark task.\n        \n        .. versionadded:: 1.6\n    \n    instr(str, substr)\n        Locate the position of the first occurrence of substr column in the given string.\n        Returns null if either of the arguments are null.\n        \n        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n            could not be found in str.\n        \n        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n        [Row(s=2)]\n        \n        .. versionadded:: 1.5\n    \n    isnan(col)\n        An expression that returns true iff the column is NaN.\n        \n        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n        \n        .. versionadded:: 1.6\n    \n    isnull(col)\n        An expression that returns true iff the column is null.\n        \n        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n        \n        .. versionadded:: 1.6\n    \n    json_tuple(col, *fields)\n        Creates a new row for a json column according to the given field names.\n        \n        :param col: string column in json format\n        :param fields: list of fields to extract\n        \n        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n        \n        .. versionadded:: 1.6\n    \n    kurtosis(col)\n        Aggregate function: returns the kurtosis of the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    lag(col, count=1, default=None)\n        Window function: returns the value that is `offset` rows before the current row, and\n        `defaultValue` if there is less than `offset` rows before the current row. For example,\n        an `offset` of one will return the previous row at any given point in the window partition.\n        \n        This is equivalent to the LAG function in SQL.\n        \n        :param col: name of column or expression\n        :param count: number of row to extend\n        :param default: default value\n        \n        .. versionadded:: 1.4\n    \n    last(col, ignorenulls=False)\n        Aggregate function: returns the last value in a group.\n        \n        The function by default returns the last values it sees. It will return the last non-null\n        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n        \n        .. note:: The function is non-deterministic because its results depends on order of rows\n            which may be non-deterministic after a shuffle.\n        \n        .. versionadded:: 1.3\n    \n    last_day(date)\n        Returns the last day of the month which the given date belongs to.\n        \n        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n        >>> df.select(last_day(df.d).alias('date')).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n        \n        .. versionadded:: 1.5\n    \n    lead(col, count=1, default=None)\n        Window function: returns the value that is `offset` rows after the current row, and\n        `defaultValue` if there is less than `offset` rows after the current row. For example,\n        an `offset` of one will return the next row at any given point in the window partition.\n        \n        This is equivalent to the LEAD function in SQL.\n        \n        :param col: name of column or expression\n        :param count: number of row to extend\n        :param default: default value\n        \n        .. versionadded:: 1.4\n    \n    least(*cols)\n        Returns the least value of the list of column names, skipping null values.\n        This function takes at least 2 parameters. It will return null iff all parameters are null.\n        \n        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n        [Row(least=1)]\n        \n        .. versionadded:: 1.5\n    \n    length(col)\n        Computes the character length of string data or number of bytes of binary data.\n        The length of character data includes the trailing spaces. The length of binary data\n        includes binary zeros.\n        \n        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n        [Row(length=4)]\n        \n        .. versionadded:: 1.5\n    \n    levenshtein(left, right)\n        Computes the Levenshtein distance of the two given strings.\n        \n        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n        [Row(d=3)]\n        \n        .. versionadded:: 1.5\n    \n    lit(col)\n        Creates a :class:`Column` of literal value.\n        \n        >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n        [Row(height=5, spark_user=True)]\n        \n        .. versionadded:: 1.3\n    \n    locate(substr, str, pos=1)\n        Locate the position of the first occurrence of substr in a string column, after position pos.\n        \n        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n            could not be found in str.\n        \n        :param substr: a string\n        :param str: a Column of :class:`pyspark.sql.types.StringType`\n        :param pos: start position (zero based)\n        \n        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n        [Row(s=2)]\n        \n        .. versionadded:: 1.5\n    \n    log(arg1, arg2=None)\n        Returns the first argument-based logarithm of the second argument.\n        \n        If there is only one argument, then this takes the natural logarithm of the argument.\n        \n        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n        ['0.30102', '0.69897']\n        \n        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n        ['0.69314', '1.60943']\n        \n        .. versionadded:: 1.5\n    \n    log10(col)\n        Computes the logarithm of the given value in Base 10.\n        \n        .. versionadded:: 1.4\n    \n    log1p(col)\n        Computes the natural logarithm of the given value plus one.\n        \n        .. versionadded:: 1.4\n    \n    log2(col)\n        Returns the base-2 logarithm of the argument.\n        \n        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n        [Row(log2=2.0)]\n        \n        .. versionadded:: 1.5\n    \n    lower(col)\n        Converts a string column to lower case.\n        \n        .. versionadded:: 1.5\n    \n    lpad(col, len, pad)\n        Left-pad the string column to width `len` with `pad`.\n        \n        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n        [Row(s='##abcd')]\n        \n        .. versionadded:: 1.5\n    \n    ltrim(col)\n        Trim the spaces from left end for the specified string value.\n        \n        .. versionadded:: 1.5\n    \n    map_concat(*cols)\n        Returns the union of all the given maps.\n        \n        :param cols: list of column names (string) or list of :class:`Column` expressions\n        \n        >>> from pyspark.sql.functions import map_concat\n        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c', 1, 'd') as map2\")\n        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n        +--------------------------------+\n        |map3                            |\n        +--------------------------------+\n        |[1 -> a, 2 -> b, 3 -> c, 1 -> d]|\n        +--------------------------------+\n        \n        .. versionadded:: 2.4\n    \n    map_from_arrays(col1, col2)\n        Creates a new map from two arrays.\n        \n        :param col1: name of column containing a set of keys. All elements should not be null\n        :param col2: name of column containing a set of values\n        \n        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n        >>> df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()\n        +----------------+\n        |             map|\n        +----------------+\n        |[2 -> a, 5 -> b]|\n        +----------------+\n        \n        .. versionadded:: 2.4\n    \n    map_from_entries(col)\n        Collection function: Returns a map created from the given array of entries.\n        \n        :param col: name of column or expression\n        \n        >>> from pyspark.sql.functions import map_from_entries\n        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n        +----------------+\n        |             map|\n        +----------------+\n        |[1 -> a, 2 -> b]|\n        +----------------+\n        \n        .. versionadded:: 2.4\n    \n    map_keys(col)\n        Collection function: Returns an unordered array containing the keys of the map.\n        \n        :param col: name of column or expression\n        \n        >>> from pyspark.sql.functions import map_keys\n        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n        +------+\n        |  keys|\n        +------+\n        |[1, 2]|\n        +------+\n        \n        .. versionadded:: 2.3\n    \n    map_values(col)\n        Collection function: Returns an unordered array containing the values of the map.\n        \n        :param col: name of column or expression\n        \n        >>> from pyspark.sql.functions import map_values\n        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n        +------+\n        |values|\n        +------+\n        |[a, b]|\n        +------+\n        \n        .. versionadded:: 2.3\n    \n    max(col)\n        Aggregate function: returns the maximum value of the expression in a group.\n        \n        .. versionadded:: 1.3\n    \n    md5(col)\n        Calculates the MD5 digest and returns the value as a 32 character hex string.\n        \n        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n        \n        .. versionadded:: 1.5\n    \n    mean(col)\n        Aggregate function: returns the average of the values in a group.\n        \n        .. versionadded:: 1.3\n    \n    min(col)\n        Aggregate function: returns the minimum value of the expression in a group.\n        \n        .. versionadded:: 1.3\n    \n    minute(col)\n        Extract the minutes of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n        >>> df.select(minute('ts').alias('minute')).collect()\n        [Row(minute=8)]\n        \n        .. versionadded:: 1.5\n    \n    monotonically_increasing_id()\n        A column that generates monotonically increasing 64-bit integers.\n        \n        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n        The current implementation puts the partition ID in the upper 31 bits, and the record number\n        within each partition in the lower 33 bits. The assumption is that the data frame has\n        less than 1 billion partitions, and each partition has less than 8 billion records.\n        \n        .. note:: The function is non-deterministic because its result depends on partition IDs.\n        \n        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n        This expression would return the following IDs:\n        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n        \n        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n        \n        .. versionadded:: 1.6\n    \n    month(col)\n         Extract the month of a given date as integer.\n        \n         >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n         >>> df.select(month('dt').alias('month')).collect()\n         [Row(month=4)]\n        \n        .. versionadded:: 1.5\n    \n    months_between(date1, date2, roundOff=True)\n        Returns number of months between dates date1 and date2.\n        If date1 is later than date2, then the result is positive.\n        If date1 and date2 are on the same day of month, or both are the last day of month,\n        returns an integer (time of day will be ignored).\n        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n        [Row(months=3.94959677)]\n        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n        [Row(months=3.9495967741935485)]\n        \n        .. versionadded:: 1.5\n    \n    nanvl(col1, col2)\n        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n        \n        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n        \n        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n        \n        .. versionadded:: 1.6\n    \n    next_day(date, dayOfWeek)\n        Returns the first date which is later than the value of the date column.\n        \n        Day of the week parameter is case insensitive, and accepts:\n            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n        \n        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n        [Row(date=datetime.date(2015, 8, 2))]\n        \n        .. versionadded:: 1.5\n    \n    ntile(n)\n        Window function: returns the ntile group id (from 1 to `n` inclusive)\n        in an ordered window partition. For example, if `n` is 4, the first\n        quarter of the rows will get value 1, the second quarter will get 2,\n        the third quarter will get 3, and the last quarter will get 4.\n        \n        This is equivalent to the NTILE function in SQL.\n        \n        :param n: an integer\n        \n        .. versionadded:: 1.4\n    \n    pandas_udf(f=None, returnType=None, functionType=None)\n        Creates a vectorized user defined function (UDF).\n        \n        :param f: user-defined function. A python function if used as a standalone function\n        :param returnType: the return type of the user-defined function. The value can be either a\n            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n                             Default: SCALAR.\n        \n        .. note:: Experimental\n        \n        The function type of the UDF can be one of the following:\n        \n        1. SCALAR\n        \n           A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\n           The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\n        \n           :class:`MapType`, :class:`StructType` are currently not supported as output types.\n        \n           Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\n           :meth:`pyspark.sql.DataFrame.select`.\n        \n           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n           >>> from pyspark.sql.types import IntegerType, StringType\n           >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\n           >>> @pandas_udf(StringType())  # doctest: +SKIP\n           ... def to_upper(s):\n           ...     return s.str.upper()\n           ...\n           >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n           ... def add_one(x):\n           ...     return x + 1\n           ...\n           >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\n           ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\n           >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\n           ...     .show()  # doctest: +SKIP\n           +----------+--------------+------------+\n           |slen(name)|to_upper(name)|add_one(age)|\n           +----------+--------------+------------+\n           |         8|      JOHN DOE|          22|\n           +----------+--------------+------------+\n        \n           .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input\n               column, but is the length of an internal batch used for each call to the function.\n               Therefore, this can be used, for example, to ensure the length of each returned\n               `pandas.Series`, and can not be used as the column length.\n        \n        2. GROUPED_MAP\n        \n           A grouped map UDF defines transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`\n           The returnType should be a :class:`StructType` describing the schema of the returned\n           `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n           the field names in the defined returnType schema if specified as strings, or match the\n           field data types by position if not strings, e.g. integer indices.\n           The length of the returned `pandas.DataFrame` can be arbitrary.\n        \n           Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.\n        \n           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n           >>> df = spark.createDataFrame(\n           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n           ...     (\"id\", \"v\"))  # doctest: +SKIP\n           >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n           ... def normalize(pdf):\n           ...     v = pdf.v\n           ...     return pdf.assign(v=(v - v.mean()) / v.std())\n           >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n           +---+-------------------+\n           | id|                  v|\n           +---+-------------------+\n           |  1|-0.7071067811865475|\n           |  1| 0.7071067811865475|\n           |  2|-0.8320502943378437|\n           |  2|-0.2773500981126146|\n           |  2| 1.1094003924504583|\n           +---+-------------------+\n        \n           Alternatively, the user can define a function that takes two arguments.\n           In this case, the grouping key(s) will be passed as the first argument and the data will\n           be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\n           data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\n           as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\n           This is useful when the user does not want to hardcode grouping key(s) in the function.\n        \n           >>> import pandas as pd  # doctest: +SKIP\n           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n           >>> df = spark.createDataFrame(\n           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n           ...     (\"id\", \"v\"))  # doctest: +SKIP\n           >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n           ... def mean_udf(key, pdf):\n           ...     # key is a tuple of one numpy.int64, which is the value\n           ...     # of 'id' for the current group\n           ...     return pd.DataFrame([key + (pdf.v.mean(),)])\n           >>> df.groupby('id').apply(mean_udf).show()  # doctest: +SKIP\n           +---+---+\n           | id|  v|\n           +---+---+\n           |  1|1.5|\n           |  2|6.0|\n           +---+---+\n           >>> @pandas_udf(\n           ...    \"id long, `ceil(v / 2)` long, v double\",\n           ...    PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n           >>> def sum_udf(key, pdf):\n           ...     # key is a tuple of two numpy.int64s, which is the values\n           ...     # of 'id' and 'ceil(df.v / 2)' for the current group\n           ...     return pd.DataFrame([key + (pdf.v.sum(),)])\n           >>> df.groupby(df.id, ceil(df.v / 2)).apply(sum_udf).show()  # doctest: +SKIP\n           +---+-----------+----+\n           | id|ceil(v / 2)|   v|\n           +---+-----------+----+\n           |  2|          5|10.0|\n           |  1|          1| 3.0|\n           |  2|          3| 5.0|\n           |  2|          2| 3.0|\n           +---+-----------+----+\n        \n           .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is\n               recommended to explicitly index the columns by name to ensure the positions are correct,\n               or alternatively use an `OrderedDict`.\n               For example, `pd.DataFrame({'id': ids, 'a': data}, columns=['id', 'a'])` or\n               `pd.DataFrame(OrderedDict([('id', ids), ('a', data)]))`.\n        \n           .. seealso:: :meth:`pyspark.sql.GroupedData.apply`\n        \n        3. GROUPED_AGG\n        \n           A grouped aggregate UDF defines a transformation: One or more `pandas.Series` -> A scalar\n           The `returnType` should be a primitive data type, e.g., :class:`DoubleType`.\n           The returned scalar can be either a python primitive type, e.g., `int` or `float`\n           or a numpy data type, e.g., `numpy.int64` or `numpy.float64`.\n        \n           :class:`MapType` and :class:`StructType` are currently not supported as output types.\n        \n           Group aggregate UDFs are used with :meth:`pyspark.sql.GroupedData.agg` and\n           :class:`pyspark.sql.Window`\n        \n           This example shows using grouped aggregated UDFs with groupby:\n        \n           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n           >>> df = spark.createDataFrame(\n           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n           ...     (\"id\", \"v\"))\n           >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n           ... def mean_udf(v):\n           ...     return v.mean()\n           >>> df.groupby(\"id\").agg(mean_udf(df['v'])).show()  # doctest: +SKIP\n           +---+-----------+\n           | id|mean_udf(v)|\n           +---+-----------+\n           |  1|        1.5|\n           |  2|        6.0|\n           +---+-----------+\n        \n           This example shows using grouped aggregated UDFs as window functions. Note that only\n           unbounded window frame is supported at the moment:\n        \n           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n           >>> from pyspark.sql import Window\n           >>> df = spark.createDataFrame(\n           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n           ...     (\"id\", \"v\"))\n           >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n           ... def mean_udf(v):\n           ...     return v.mean()\n           >>> w = Window \\\n           ...     .partitionBy('id') \\\n           ...     .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n           >>> df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()  # doctest: +SKIP\n           +---+----+------+\n           | id|   v|mean_v|\n           +---+----+------+\n           |  1| 1.0|   1.5|\n           |  1| 2.0|   1.5|\n           |  2| 3.0|   6.0|\n           |  2| 5.0|   6.0|\n           |  2|10.0|   6.0|\n           +---+----+------+\n        \n           .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`\n        \n        .. note:: The user-defined functions are considered deterministic by default. Due to\n            optimization, duplicate invocations may be eliminated or the function may even be invoked\n            more times than it is present in the query. If your function is not deterministic, call\n            `asNondeterministic` on the user defined function. E.g.:\n        \n        >>> @pandas_udf('double', PandasUDFType.SCALAR)  # doctest: +SKIP\n        ... def random(v):\n        ...     import numpy as np\n        ...     import pandas as pd\n        ...     return pd.Series(np.random.randn(len(v))\n        >>> random = random.asNondeterministic()  # doctest: +SKIP\n        \n        .. note:: The user-defined functions do not support conditional expressions or short circuiting\n            in boolean expressions and it ends up with being executed all internally. If the functions\n            can fail on special rows, the workaround is to incorporate the condition into the functions.\n        \n        .. note:: The user-defined functions do not take keyword arguments on the calling side.\n        \n        .. versionadded:: 2.3\n    \n    percent_rank()\n        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n        \n        .. versionadded:: 1.6\n    \n    posexplode(col)\n        Returns a new row for each element with position in the given array or map.\n        \n        >>> from pyspark.sql import Row\n        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n        >>> eDF.select(posexplode(eDF.intlist)).collect()\n        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n        \n        >>> eDF.select(posexplode(eDF.mapfield)).show()\n        +---+---+-----+\n        |pos|key|value|\n        +---+---+-----+\n        |  0|  a|    b|\n        +---+---+-----+\n        \n        .. versionadded:: 2.1\n    \n    posexplode_outer(col)\n        Returns a new row for each element with position in the given array or map.\n        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n        \n        >>> df = spark.createDataFrame(\n        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n        ...     (\"id\", \"an_array\", \"a_map\")\n        ... )\n        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n        +---+----------+----+----+-----+\n        | id|  an_array| pos| key|value|\n        +---+----------+----+----+-----+\n        |  1|[foo, bar]|   0|   x|  1.0|\n        |  2|        []|null|null| null|\n        |  3|      null|null|null| null|\n        +---+----------+----+----+-----+\n        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n        +---+----------+----+----+\n        | id|     a_map| pos| col|\n        +---+----------+----+----+\n        |  1|[x -> 1.0]|   0| foo|\n        |  1|[x -> 1.0]|   1| bar|\n        |  2|        []|null|null|\n        |  3|      null|null|null|\n        +---+----------+----+----+\n        \n        .. versionadded:: 2.3\n    \n    pow(col1, col2)\n        Returns the value of the first argument raised to the power of the second argument.\n        \n        .. versionadded:: 1.4\n    \n    quarter(col)\n        Extract the quarter of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(quarter('dt').alias('quarter')).collect()\n        [Row(quarter=2)]\n        \n        .. versionadded:: 1.5\n    \n    radians(col)\n        Converts an angle measured in degrees to an approximately equivalent angle\n        measured in radians.\n        :param col: angle in degrees\n        :return: angle in radians, as if computed by `java.lang.Math.toRadians()`\n        \n        .. versionadded:: 2.1\n    \n    rand(seed=None)\n        Generates a random column with independent and identically distributed (i.i.d.) samples\n        from U[0.0, 1.0].\n        \n        .. note:: The function is non-deterministic in general case.\n        \n        >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n        [Row(age=2, name='Alice', rand=1.1568609015300986),\n         Row(age=5, name='Bob', rand=1.403379671529166)]\n        \n        .. versionadded:: 1.4\n    \n    randn(seed=None)\n        Generates a column with independent and identically distributed (i.i.d.) samples from\n        the standard normal distribution.\n        \n        .. note:: The function is non-deterministic in general case.\n        \n        >>> df.withColumn('randn', randn(seed=42)).collect()\n        [Row(age=2, name='Alice', randn=-0.7556247885860078),\n        Row(age=5, name='Bob', randn=-0.0861619008451133)]\n        \n        .. versionadded:: 1.4\n    \n    rank()\n        Window function: returns the rank of rows within a window partition.\n        \n        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n        and had three people tie for second place, you would say that all three were in second\n        place and that the next person came in third. Rank would give me sequential numbers, making\n        the person that came in third place (after the ties) would register as coming in fifth.\n        \n        This is equivalent to the RANK function in SQL.\n        \n        .. versionadded:: 1.6\n    \n    regexp_extract(str, pattern, idx)\n        Extract a specific group matched by a Java regex, from the specified string column.\n        If the regex did not match, or the specified group did not match, an empty string is returned.\n        \n        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n        [Row(d='100')]\n        >>> df = spark.createDataFrame([('foo',)], ['str'])\n        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n        [Row(d='')]\n        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n        [Row(d='')]\n        \n        .. versionadded:: 1.5\n    \n    regexp_replace(str, pattern, replacement)\n        Replace all substrings of the specified string value that match regexp with rep.\n        \n        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n        [Row(d='-----')]\n        \n        .. versionadded:: 1.5\n    \n    repeat(col, n)\n        Repeats a string column n times, and returns it as a new string column.\n        \n        >>> df = spark.createDataFrame([('ab',)], ['s',])\n        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n        [Row(s='ababab')]\n        \n        .. versionadded:: 1.5\n    \n    reverse(col)\n        Collection function: returns a reversed string or an array with reverse order of elements.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n        >>> df.select(reverse(df.data).alias('s')).collect()\n        [Row(s='LQS krapS')]\n        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n        >>> df.select(reverse(df.data).alias('r')).collect()\n        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n        \n        .. versionadded:: 1.5\n    \n    rint(col)\n        Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\n        \n        .. versionadded:: 1.4\n    \n    round(col, scale=0)\n        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n        or at integral part when `scale` < 0.\n        \n        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n        [Row(r=3.0)]\n        \n        .. versionadded:: 1.5\n    \n    row_number()\n        Window function: returns a sequential number starting at 1 within a window partition.\n        \n        .. versionadded:: 1.6\n    \n    rpad(col, len, pad)\n        Right-pad the string column to width `len` with `pad`.\n        \n        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n        [Row(s='abcd##')]\n        \n        .. versionadded:: 1.5\n    \n    rtrim(col)\n        Trim the spaces from right end for the specified string value.\n        \n        .. versionadded:: 1.5\n    \n    schema_of_json(json)\n        Parses a JSON string and infers its schema in DDL format.\n        \n        :param json: a JSON string or a string literal containing a JSON string.\n        \n        >>> df = spark.range(1)\n        >>> df.select(schema_of_json('{\"a\": 0}').alias(\"json\")).collect()\n        [Row(json='struct<a:bigint>')]\n        \n        .. versionadded:: 2.4\n    \n    second(col)\n        Extract the seconds of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n        >>> df.select(second('ts').alias('second')).collect()\n        [Row(second=15)]\n        \n        .. versionadded:: 1.5\n    \n    sequence(start, stop, step=None)\n        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n        otherwise -1.\n        \n        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n        [Row(r=[-2, -1, 0, 1, 2])]\n        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n        [Row(r=[4, 2, 0, -2, -4])]\n        \n        .. versionadded:: 2.4\n    \n    sha1(col)\n        Returns the hex string result of SHA-1.\n        \n        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n        \n        .. versionadded:: 1.5\n    \n    sha2(col, numBits)\n        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n        \n        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n        >>> digests[0]\n        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n        >>> digests[1]\n        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n        \n        .. versionadded:: 1.5\n    \n    shiftLeft(col, numBits)\n        Shift the given value numBits left.\n        \n        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n        [Row(r=42)]\n        \n        .. versionadded:: 1.5\n    \n    shiftRight(col, numBits)\n        (Signed) shift the given value numBits right.\n        \n        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n        [Row(r=21)]\n        \n        .. versionadded:: 1.5\n    \n    shiftRightUnsigned(col, numBits)\n        Unsigned shift the given value numBits right.\n        \n        >>> df = spark.createDataFrame([(-42,)], ['a'])\n        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n        [Row(r=9223372036854775787)]\n        \n        .. versionadded:: 1.5\n    \n    shuffle(col)\n        Collection function: Generates a random permutation of the given array.\n        \n        .. note:: The function is non-deterministic.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n        \n        .. versionadded:: 2.4\n    \n    signum(col)\n        Computes the signum of the given value.\n        \n        .. versionadded:: 1.4\n    \n    sin(col)\n        :param col: angle in radians\n        :return: sine of the angle, as if computed by `java.lang.Math.sin()`\n        \n        .. versionadded:: 1.4\n    \n    sinh(col)\n        :param col: hyperbolic angle\n        :return: hyperbolic sine of the given value,\n                 as if computed by `java.lang.Math.sinh()`\n        \n        .. versionadded:: 1.4\n    \n    size(col)\n        Collection function: returns the length of the array or map stored in the column.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n        >>> df.select(size(df.data)).collect()\n        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n        \n        .. versionadded:: 1.5\n    \n    skewness(col)\n        Aggregate function: returns the skewness of the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    slice(x, start, length)\n        Collection function: returns an array containing  all the elements in `x` from index `start`\n        (or starting from the end if `start` is negative) with the specified `length`.\n        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n        [Row(sliced=[2, 3]), Row(sliced=[5])]\n        \n        .. versionadded:: 2.4\n    \n    sort_array(col, asc=True)\n        Collection function: sorts the input array in ascending or descending order according\n        to the natural ordering of the array elements. Null elements will be placed at the beginning\n        of the returned array in ascending order or at the end of the returned array in descending\n        order.\n        \n        :param col: name of column or expression\n        \n        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n        >>> df.select(sort_array(df.data).alias('r')).collect()\n        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n        \n        .. versionadded:: 1.5\n    \n    soundex(col)\n        Returns the SoundEx encoding for a string\n        \n        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n        [Row(soundex='P362'), Row(soundex='U612')]\n        \n        .. versionadded:: 1.5\n    \n    spark_partition_id()\n        A column for partition ID.\n        \n        .. note:: This is indeterministic because it depends on data partitioning and task scheduling.\n        \n        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n        [Row(pid=0), Row(pid=0)]\n        \n        .. versionadded:: 1.6\n    \n    split(str, pattern)\n        Splits str around pattern (pattern is a regular expression).\n        \n        .. note:: pattern is a string represent the regular expression.\n        \n        >>> df = spark.createDataFrame([('ab12cd',)], ['s',])\n        >>> df.select(split(df.s, '[0-9]+').alias('s')).collect()\n        [Row(s=['ab', 'cd'])]\n        \n        .. versionadded:: 1.5\n    \n    sqrt(col)\n        Computes the square root of the specified float value.\n        \n        .. versionadded:: 1.3\n    \n    stddev(col)\n        Aggregate function: alias for stddev_samp.\n        \n        .. versionadded:: 1.6\n    \n    stddev_pop(col)\n        Aggregate function: returns population standard deviation of the expression in a group.\n        \n        .. versionadded:: 1.6\n    \n    stddev_samp(col)\n        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n        \n        .. versionadded:: 1.6\n    \n    struct(*cols)\n        Creates a new struct column.\n        \n        :param cols: list of column names (string) or list of :class:`Column` expressions\n        \n        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n        \n        .. versionadded:: 1.4\n    \n    substring(str, pos, len)\n        Substring starts at `pos` and is of length `len` when str is String type or\n        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n        when str is Binary type.\n        \n        .. note:: The position is not zero based, but 1 based index.\n        \n        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n        [Row(s='ab')]\n        \n        .. versionadded:: 1.5\n    \n    substring_index(str, delim, count)\n        Returns the substring from string str before count occurrences of the delimiter delim.\n        If count is positive, everything the left of the final delimiter (counting from left) is\n        returned. If count is negative, every to the right of the final delimiter (counting from the\n        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n        \n        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n        [Row(s='a.b')]\n        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n        [Row(s='b.c.d')]\n        \n        .. versionadded:: 1.5\n    \n    sum(col)\n        Aggregate function: returns the sum of all values in the expression.\n        \n        .. versionadded:: 1.3\n    \n    sumDistinct(col)\n        Aggregate function: returns the sum of distinct values in the expression.\n        \n        .. versionadded:: 1.3\n    \n    tan(col)\n        :param col: angle in radians\n        :return: tangent of the given value, as if computed by `java.lang.Math.tan()`\n        \n        .. versionadded:: 1.4\n    \n    tanh(col)\n        :param col: hyperbolic angle\n        :return: hyperbolic tangent of the given value,\n                 as if computed by `java.lang.Math.tanh()`\n        \n        .. versionadded:: 1.4\n    \n    toDegrees(col)\n        .. note:: Deprecated in 2.1, use :func:`degrees` instead.\n        \n        .. versionadded:: 1.4\n    \n    toRadians(col)\n        .. note:: Deprecated in 2.1, use :func:`radians` instead.\n        \n        .. versionadded:: 1.4\n    \n    to_date(col, format=None)\n        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n        using the optionally specified format. Specify formats according to\n        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n        is omitted (equivalent to ``col.cast(\"date\")``).\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n        >>> df.select(to_date(df.t).alias('date')).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n        [Row(date=datetime.date(1997, 2, 28))]\n        \n        .. versionadded:: 2.2\n    \n    to_json(col, options={})\n        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n        into a JSON string. Throws an exception, in the case of an unsupported type.\n        \n        :param col: name of column containing a struct, an array or a map.\n        :param options: options to control converting. accepts the same options as the JSON datasource\n        \n        >>> from pyspark.sql import Row\n        >>> from pyspark.sql.types import *\n        >>> data = [(1, Row(name='Alice', age=2))]\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n        >>> data = [(1, [Row(name='Alice', age=2), Row(name='Bob', age=3)])]\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n        >>> data = [(1, {\"name\": \"Alice\"})]\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n        [Row(json='{\"name\":\"Alice\"}')]\n        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n        >>> data = [(1, [\"Alice\", \"Bob\"])]\n        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n        [Row(json='[\"Alice\",\"Bob\"]')]\n        \n        .. versionadded:: 2.1\n    \n    to_timestamp(col, format=None)\n        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n        using the optionally specified format. Specify formats according to\n        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n        is omitted (equivalent to ``col.cast(\"timestamp\")``).\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n        \n        .. versionadded:: 2.2\n    \n    to_utc_timestamp(timestamp, tz)\n        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n        timezone, and renders that timestamp as a timestamp in UTC.\n        \n        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n        timezone to UTC timezone.\n        \n        This function may return confusing result if the input is a string with timezone, e.g.\n        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n        according to the timezone in the string, and finally display the result by converting the\n        timestamp to string according to the session local timezone.\n        \n        :param timestamp: the column that contains timestamps\n        :param tz: a string that has the ID of timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc\n        \n        .. versionchanged:: 2.4\n           `tz` can take a :class:`Column` containing timezone ID strings.\n        \n        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n        \n        .. versionadded:: 1.5\n    \n    translate(srcCol, matching, replace)\n        A function translate any character in the `srcCol` by a character in `matching`.\n        The characters in `replace` is corresponding to the characters in `matching`.\n        The translate will happen when any character in the string matching with the character\n        in the `matching`.\n        \n        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n        ...     .alias('r')).collect()\n        [Row(r='1a2s3ae')]\n        \n        .. versionadded:: 1.5\n    \n    trim(col)\n        Trim the spaces from both ends for the specified string column.\n        \n        .. versionadded:: 1.5\n    \n    trunc(date, format)\n        Returns date truncated to the unit specified by the format.\n        \n        :param format: 'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'\n        \n        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n        [Row(year=datetime.date(1997, 1, 1))]\n        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n        [Row(month=datetime.date(1997, 2, 1))]\n        \n        .. versionadded:: 1.5\n    \n    udf(f=None, returnType=StringType)\n        Creates a user defined function (UDF).\n        \n        .. note:: The user-defined functions are considered deterministic by default. Due to\n            optimization, duplicate invocations may be eliminated or the function may even be invoked\n            more times than it is present in the query. If your function is not deterministic, call\n            `asNondeterministic` on the user defined function. E.g.:\n        \n        >>> from pyspark.sql.types import IntegerType\n        >>> import random\n        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n        \n        .. note:: The user-defined functions do not support conditional expressions or short circuiting\n            in boolean expressions and it ends up with being executed all internally. If the functions\n            can fail on special rows, the workaround is to incorporate the condition into the functions.\n        \n        .. note:: The user-defined functions do not take keyword arguments on the calling side.\n        \n        :param f: python function if used as a standalone function\n        :param returnType: the return type of the user-defined function. The value can be either a\n            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        \n        >>> from pyspark.sql.types import IntegerType\n        >>> slen = udf(lambda s: len(s), IntegerType())\n        >>> @udf\n        ... def to_upper(s):\n        ...     if s is not None:\n        ...         return s.upper()\n        ...\n        >>> @udf(returnType=IntegerType())\n        ... def add_one(x):\n        ...     if x is not None:\n        ...         return x + 1\n        ...\n        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n        +----------+--------------+------------+\n        |slen(name)|to_upper(name)|add_one(age)|\n        +----------+--------------+------------+\n        |         8|      JOHN DOE|          22|\n        +----------+--------------+------------+\n        \n        .. versionadded:: 1.3\n    \n    unbase64(col)\n        Decodes a BASE64 encoded string column and returns it as a binary column.\n        \n        .. versionadded:: 1.5\n    \n    unhex(col)\n        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n        and converts to the byte representation of number.\n        \n        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n        [Row(unhex(a)=bytearray(b'ABC'))]\n        \n        .. versionadded:: 1.5\n    \n    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n        to Unix time stamp (in seconds), using the default timezone and the default\n        locale, return null if fail.\n        \n        if `timestamp` is None, then it returns current timestamp.\n        \n        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n        [Row(unix_time=1428476400)]\n        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n        \n        .. versionadded:: 1.5\n    \n    upper(col)\n        Converts a string column to upper case.\n        \n        .. versionadded:: 1.5\n    \n    var_pop(col)\n        Aggregate function: returns the population variance of the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    var_samp(col)\n        Aggregate function: returns the unbiased sample variance of the values in a group.\n        \n        .. versionadded:: 1.6\n    \n    variance(col)\n        Aggregate function: alias for var_samp.\n        \n        .. versionadded:: 1.6\n    \n    weekofyear(col)\n        Extract the week number of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n        [Row(week=15)]\n        \n        .. versionadded:: 1.5\n    \n    when(condition, value)\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n        \n        :param condition: a boolean :class:`Column` expression.\n        :param value: a literal value, or a :class:`Column` expression.\n        \n        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n        [Row(age=3), Row(age=4)]\n        \n        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n        [Row(age=3), Row(age=None)]\n        \n        .. versionadded:: 1.4\n    \n    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n        the order of months are not supported.\n        \n        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n        \n        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n        \n        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n        \n        The output column will be a struct called 'window' by default with the nested columns 'start'\n        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n        \n        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n        \n        .. versionadded:: 2.0\n    \n    year(col)\n        Extract the year of a given date as integer.\n        \n        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n        >>> df.select(year('dt').alias('year')).collect()\n        [Row(year=2015)]\n        \n        .. versionadded:: 1.5\n\nDATA\n    __all__ = ['PandasUDFType', 'abs', 'acos', 'add_months', 'approxCountD...\n\nFILE\n    /home/nbuser/anaconda3_501/lib/python3.6/site-packages/pyspark/sql/functions.py\n\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Better use full notation when mentioning the column names"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# substring"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(substring('order_date',1,7).alias('Month'), substring('order_date',9,2).alias('day')).show(5)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+-------+---+\n|  Month|day|\n+-------+---+\n|2013-07| 25|\n|2013-07| 25|\n|2013-07| 25|\n|2013-07| 25|\n|2013-07| 25|\n+-------+---+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(orders.order_date.substr(1,7).alias('Month'), orders.order_date.substr(9,2).alias('Day')).show(5)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+-------+---+\n|  Month|Day|\n+-------+---+\n|2013-07| 25|\n|2013-07| 25|\n|2013-07| 25|\n|2013-07| 25|\n|2013-07| 25|\n+-------+---+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## substring_index"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(substring_index('order_date','-',2)).show(5)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------------------+\n|substring_index(order_date, -, 2)|\n+---------------------------------+\n|                          2013-07|\n|                          2013-07|\n|                          2013-07|\n|                          2013-07|\n|                          2013-07|\n+---------------------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## instr"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(instr('order_date','07')).show(5)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------+\n|instr(order_date, 07)|\n+---------------------+\n|                    6|\n|                    6|\n|                    6|\n|                    6|\n|                    6|\n+---------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## split"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(split('order_date','-')[0].alias('year')).show(5)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+----+\n|year|\n+----+\n|2013|\n|2013|\n|2013|\n|2013|\n|2013|\n+----+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## concat"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(concat('order_status',lit(','),'order_id')).show(5)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------------------+\n|concat(order_status, ,, order_id)|\n+---------------------------------+\n|                         CLOSED,1|\n|                PENDING_PAYMENT,2|\n|                       COMPLETE,3|\n|                         CLOSED,4|\n|                       COMPLETE,5|\n+---------------------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## concat_ws() \n### is very important for concatenating the columns with field delimiter"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(concat_ws('!','order_status','order_id','order_customer_id')).show(5)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+-------------------------------------------------------+\n|concat_ws(!, order_status, order_id, order_customer_id)|\n+-------------------------------------------------------+\n|                                         CLOSED!1!11599|\n|                                   PENDING_PAYMENT!2...|\n|                                       COMPLETE!3!12111|\n|                                          CLOSED!4!8827|\n|                                       COMPLETE!5!11318|\n+-------------------------------------------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## reverse"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(reverse('order_status')).show(5)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------+\n|reverse(order_status)|\n+---------------------+\n|               DESOLC|\n|      TNEMYAP_GNIDNEP|\n|             ETELPMOC|\n|               DESOLC|\n|             ETELPMOC|\n+---------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## length"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(length('order_id')).show(5)",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+----------------+\n|length(order_id)|\n+----------------+\n|               1|\n|               1|\n|               1|\n|               1|\n|               1|\n+----------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# when"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select('order_id',when(orders.order_id % 2 == 0, 'EVEN').otherwise('ODD')).show(5)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-----------------------------------------------------+\n|order_id|CASE WHEN ((order_id % 2) = 0) THEN EVEN ELSE ODD END|\n+--------+-----------------------------------------------------+\n|       1|                                                  ODD|\n|       2|                                                 EVEN|\n|       3|                                                  ODD|\n|       4|                                                 EVEN|\n|       5|                                                  ODD|\n+--------+-----------------------------------------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# repeat"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(repeat('order_id',10)).show(5)",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------------------+\n|repeat(order_id, 10)|\n+--------------------+\n|          1111111111|\n|          2222222222|\n|          3333333333|\n|          4444444444|\n|          5555555555|\n+--------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## rpad"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(rpad('order_id',15,'*')).show(5)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------+\n|rpad(order_id, 15, *)|\n+---------------------+\n|      1**************|\n|      2**************|\n|      3**************|\n|      4**************|\n|      5**************|\n+---------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## lpad"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(lpad('order_id',15,'*')).show(5)",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------+\n|lpad(order_id, 15, *)|\n+---------------------+\n|      **************1|\n|      **************2|\n|      **************3|\n|      **************4|\n|      **************5|\n+---------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## lower"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(lower(orders.order_status)).show(5)",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+-------------------+\n|lower(order_status)|\n+-------------------+\n|             closed|\n|    pending_payment|\n|           complete|\n|             closed|\n|           complete|\n+-------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## upper"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(upper(orders.order_status)).show(5)",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+-------------------+\n|upper(order_status)|\n+-------------------+\n|             CLOSED|\n|    PENDING_PAYMENT|\n|           COMPLETE|\n|             CLOSED|\n|           COMPLETE|\n+-------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## initcap"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(initcap('order_status')).show(5)",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---------------------+\n|initcap(order_status)|\n+---------------------+\n|               Closed|\n|      Pending_payment|\n|             Complete|\n|               Closed|\n|             Complete|\n+---------------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## lit"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.select(lit(0),lit('Anything')).show(5)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+---+--------+\n|  0|Anything|\n+---+--------+\n|  0|Anything|\n|  0|Anything|\n|  0|Anything|\n|  0|Anything|\n|  0|Anything|\n+---+--------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## like"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.like('44%')).show(5)",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|      44|2013-07-25 00:00:00|            10500|        PENDING|\n|     440|2013-07-27 00:00:00|             7290|       COMPLETE|\n|     441|2013-07-27 00:00:00|             5239|PENDING_PAYMENT|\n|     442|2013-07-27 00:00:00|             8098|       COMPLETE|\n|     443|2013-07-27 00:00:00|             8499|         CLOSED|\n+--------+-------------------+-----------------+---------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.like('%44')).show(5)",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|      44|2013-07-25 00:00:00|            10500|        PENDING|\n|     144|2013-07-26 00:00:00|             2158|     PROCESSING|\n|     244|2013-07-26 00:00:00|             6910|PENDING_PAYMENT|\n|     344|2013-07-26 00:00:00|             2816|        PENDING|\n|     444|2013-07-27 00:00:00|            10004|       COMPLETE|\n+--------+-------------------+-----------------+---------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.like('____4')).show(5)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|   10004|2013-09-25 00:00:00|             7768|         CLOSED|\n|   10014|2013-09-25 00:00:00|            10864|SUSPECTED_FRAUD|\n|   10024|2013-09-25 00:00:00|             9678|PENDING_PAYMENT|\n|   10034|2013-09-25 00:00:00|             4554|         CLOSED|\n|   10044|2013-09-25 00:00:00|             2333|        PENDING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## contains"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.contains('444')).show(5)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+------------+\n|order_id|         order_date|order_customer_id|order_status|\n+--------+-------------------+-----------------+------------+\n|     444|2013-07-27 00:00:00|            10004|    COMPLETE|\n|    1444|2013-08-01 00:00:00|             8302|    COMPLETE|\n|    2444|2013-08-07 00:00:00|             9714|  PROCESSING|\n|    3444|2013-08-14 00:00:00|             8549|    COMPLETE|\n|    4440|2013-08-20 00:00:00|             5434|    COMPLETE|\n+--------+-------------------+-----------------+------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## endswith"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.endswith('444')).show(5)",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+------------+\n|order_id|         order_date|order_customer_id|order_status|\n+--------+-------------------+-----------------+------------+\n|     444|2013-07-27 00:00:00|            10004|    COMPLETE|\n|    1444|2013-08-01 00:00:00|             8302|    COMPLETE|\n|    2444|2013-08-07 00:00:00|             9714|  PROCESSING|\n|    3444|2013-08-14 00:00:00|             8549|    COMPLETE|\n|    4444|2013-08-20 00:00:00|            11386|    COMPLETE|\n+--------+-------------------+-----------------+------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## startswith"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.startswith('444')).show(5)",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+------------+\n|order_id|         order_date|order_customer_id|order_status|\n+--------+-------------------+-----------------+------------+\n|     444|2013-07-27 00:00:00|            10004|    COMPLETE|\n|    4440|2013-08-20 00:00:00|             5434|    COMPLETE|\n|    4441|2013-08-20 00:00:00|            10735|      CLOSED|\n|    4442|2013-08-20 00:00:00|             6820|  PROCESSING|\n|    4443|2013-08-20 00:00:00|             3817|     ON_HOLD|\n+--------+-------------------+-----------------+------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## isNotNull"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.isNotNull()).show(5)",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n+--------+-------------------+-----------------+---------------+\nonly showing top 5 rows\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## isNull"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.isNull()).show(5)",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+----------+-----------------+------------+\n|order_id|order_date|order_customer_id|order_status|\n+--------+----------+-----------------+------------+\n+--------+----------+-----------------+------------+\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## isin"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "orders.filter(orders.order_id.isin('1','2','3333',798)).show(5)",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|     798|2013-07-29 00:00:00|             8709|       COMPLETE|\n|    3333|2013-08-12 00:00:00|            10717|         CLOSED|\n+--------+-------------------+-----------------+---------------+\n\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}